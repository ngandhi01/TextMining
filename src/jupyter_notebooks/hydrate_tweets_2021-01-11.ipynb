{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of Script\n",
    "\n",
    "In this script, I'll build off the previous hydrate scripts and hydrate tweets from 2020-12-22 to 2021-01-10. This should line up with news about the new COVID strain from England as well as vaccine-related news. The tweets will be sourced from https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime as datetime\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "pd.set_option('display.max_columns', None) # show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hydrate tweets\n",
    "In this part of the code, we'll take the .csv files from the website above and get the IDs. We'll do this for all the IDs, then export all the IDs from December 22nd to January 11th as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_ID_DIR = \"../../data/tweets/tweet_ids/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_to_hydrate(link):\n",
    "    \n",
    "    \"\"\"\n",
    "        Takes the links to both of the csv files for the given date, as well as name of export file\n",
    "        \n",
    "        Assumes that directory for tweet IDs is specified\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(link, names=[\"tweet_id\", \"sentiment_score\"])\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    tweet_ids = list(df[\"tweet_id\"])\n",
    "    \n",
    "    return tweet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet_IDs(tweet_ids, filepath):\n",
    "    \"\"\"\n",
    "        Takes list of tweet IDs, exports as .csv\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filepath, \"a+\") as f:\n",
    "        for idx, tweet in enumerate(tweet_ids):\n",
    "            if idx != len(tweet_ids) - 1:\n",
    "                f.write(f\"{tweet}, \\n\")\n",
    "            else:\n",
    "                f.write(f\"{tweet}\")\n",
    "                \n",
    "    print(f\"CSV file successfully exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_list = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/december21_december22.csv?response-content-disposition=attachment%3B%20filename%3D%22december21_december22.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=607f3b90fef5da9626ddd82e5c2361c57f6f8e5cc9ecbcb2a23f46dac5caeccf\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december22_december23.csv?response-content-disposition=attachment%3B%20filename%3D%22december22_december23.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=ec8acbb3ccd2dd3e099632b6bd5dec39d35feca56a9caac975499236bd3a6c87\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december23_december24.csv?response-content-disposition=attachment%3B%20filename%3D%22december23_december24.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=b974795c225f98049c82e637990468413f66c6ca80e4833029e68bd7e91c80e2\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december24_december25.csv?response-content-disposition=attachment%3B%20filename%3D%22december24_december25.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=3c0978afccb0388ff13fe1ff61052c904251ba4447465a580f109b21d714d698\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december25_december26.csv?response-content-disposition=attachment%3B%20filename%3D%22december25_december26.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=3b760e6cc10f7fc8d66bbdfc37bad9f677fa001c099fafe700a56b1e36e56a26\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december26_december27.csv?response-content-disposition=attachment%3B%20filename%3D%22december26_december27.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=14409a2f99696c8f1a8ab90fecf5a0fefe587d43aabb5aef0d197aefc42ac137\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december27_december28.csv?response-content-disposition=attachment%3B%20filename%3D%22december27_december28.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=3cc145bddf4eb7d8eac5273901c1f7af73018ca1dbbc7b7fb4ee14102f24eda7\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december28_december29.csv?response-content-disposition=attachment%3B%20filename%3D%22december28_december29.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=1e86cf986d021a3cbbd360f15fa74f0ad57431fc8ce39a4fbd4697e415fbed9a\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december29_december30.csv?response-content-disposition=attachment%3B%20filename%3D%22december29_december30.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=4d66f267117abae9c56e9c96cce22d42376b8a941bd8b6816203ec263cf3eabb\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december30_december31.csv?response-content-disposition=attachment%3B%20filename%3D%22december30_december31.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=23843c165f6cd122a22f705119bb178927f9847949193a6ee822d29f48a73497\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/december31_january1.csv?response-content-disposition=attachment%3B%20filename%3D%22december31_january1.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=48361c629ee064426bc6f4c199318d2d9f831082713d12ffbde9d71ef1f1f283\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january1_january2.csv?response-content-disposition=attachment%3B%20filename%3D%22january1_january2.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=cf8cff84bf89a487aa483f731fe39b08c835f64840aaaa0802a0d786adc3b220\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january2_january3.csv?response-content-disposition=attachment%3B%20filename%3D%22january2_january3.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=0e9a099b0fb3bccc316326e4b1fbf0b1ce43aee048a3a626e6772b6bf4b162ad\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january3_january4.csv?response-content-disposition=attachment%3B%20filename%3D%22january3_january4.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=387d4c2111e776d9811255bc9144411ddaf79680c7cc0a18611198b7112bb645\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january4_january5.csv?response-content-disposition=attachment%3B%20filename%3D%22january4_january5.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=4052f80aa36eac3513134ae5e669adfcb984d98747ddaa0f11234d65d51f4fff\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january5_january6.csv?response-content-disposition=attachment%3B%20filename%3D%22january5_january6.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=87cc104e237a442db620be49a05a0031d26da3ef32b8c7fc67a57a40619635c9\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january6_january7.csv?response-content-disposition=attachment%3B%20filename%3D%22january6_january7.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=b124da74209b9a132248debc51a7811a21518509bd66169730007f1029facb99\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january7_january8.csv?response-content-disposition=attachment%3B%20filename%3D%22january7_january8.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=df111623cc460870fcf5642bbbc2d5acad64cd7a75d9547dd02074ded3bae9fd\", \n",
    "              \"https://ieee-dataport.s3.amazonaws.com/open/14206/january8_january9.csv?response-content-disposition=attachment%3B%20filename%3D%22january8_january9.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210111T151953Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=a38b30deac63e489a173255d830bb5073367fef6dfd2e6d2dcbfc9a6d0d4c76f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lst in links_list:\n",
    "    ids_to_add = get_tweets_to_hydrate(lst)\n",
    "    for elem in ids_to_add:\n",
    "        IDs_list.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully exported\n"
     ]
    }
   ],
   "source": [
    "save_tweet_IDs(IDs_list, TWEET_ID_DIR + \"tweet_IDs_2020-12-22_2021-01-09.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these tweet IDs, let's hydrate them to recover the original tweets\n",
    "\n",
    "First, you have to confirm your credentials.\n",
    "\n",
    "`twarc configure`\n",
    "\n",
    "Then, submit the creds. After doing so successfully, you should get a message like this:\n",
    "\n",
    "`The credentials for default have been saved to your configuration file at /Users/mark/.twarc`\n",
    "\n",
    "Afterwards, you can start hydrating the tweets.\n",
    "\n",
    "This can be done in the command line\n",
    "\n",
    "You'd run something like this:\n",
    "\n",
    "`twarc hydrate ids.txt > tweets.jsonl`\n",
    "\n",
    "In my case, running the command from the root directory of this project, it looks something like this:\n",
    "\n",
    "`twarc hydrate data/tweets/tweet_ids/tweet_IDs_2020-12-22_2021-01-09.csv > data/tweets/hydrated_tweets/2020-12-22_2021-01-09_tweets.jsonl`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocess Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_location(place):\n",
    "    \"\"\"\n",
    "    Gets state info from place field\n",
    "    Assumes dict input\n",
    "    \"\"\"\n",
    "    \n",
    "    if place is None:\n",
    "        state = \"NA\"  \n",
    "    elif place[\"country_code\"] != \"US\":\n",
    "        state = \"NA\"\n",
    "    else:\n",
    "        state = place[\"full_name\"].split(\",\")[1].strip() # e.g., \"Los Angeles, CA\" --> \"CA\"\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION ='''!()-[]{};:'\"\\,<>./?@$%^&*_~''' # keep hashtags\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "        Removes emojis\n",
    "    \"\"\"\n",
    "    text = string.encode(\"utf-8\")\n",
    "    allchars = [str for str in text.decode('utf-8')]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Removes punctuation, does string split, and removes links\n",
    "    \"\"\"\n",
    "    \n",
    "    return_arr = []\n",
    "    \n",
    "    # remove punctuation\n",
    "    text_no_punctuation = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char not in PUNCTUATION:\n",
    "            text_no_punctuation = text_no_punctuation + char\n",
    "            \n",
    "    # remove emojis\n",
    "    text_no_punctuation = remove_emoji(text_no_punctuation)\n",
    "    text_no_punctuation = re.sub(r'\\\\U[a-zA-Z0-9]{8}', '', text_no_punctuation)\n",
    "    \n",
    "    # remove \\n and \\t\n",
    "    text_no_punctuation = re.sub(r'\\n', '', text_no_punctuation)\n",
    "    text_no_punctuation = re.sub(r'\\t', '', text_no_punctuation)\n",
    "    \n",
    "    # remove escape sequences\n",
    "    text_no_escape = \"\"\n",
    "    \n",
    "    for char in text_no_punctuation:\n",
    "        try:\n",
    "            char.encode('ascii')\n",
    "            text_no_escape = text_no_escape + char # this'll catch chars that don't have an ascii equivalent (e.g., emojis)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # add space between # and another char before it (e.g., split yes#baseball into yes #baseball)\n",
    "    text_no_escape = re.sub(r\"([a-zA-Z0-9]){1}#\", r\"\\1 #\", text_no_escape)\n",
    "    \n",
    "    # other preprocessing\n",
    "    text_arr = text_no_escape.split(' ')\n",
    "    \n",
    "    for word in text_arr:\n",
    "        \n",
    "        # clean words\n",
    "        word = word.lower()\n",
    "        \n",
    "        if \"http\" not in word and word.strip() != '' and word not in STOPWORDS:\n",
    "            return_arr.append(word)\n",
    "            \n",
    "    return return_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hydrated_tweets(tweet_jsonl_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        Takes .jsonl from Twitter, returns the cleaned df\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # get uncleaned df from json\n",
    "    df = pd.read_json(path_or_buf=tweet_jsonl_path, lines=True)\n",
    "    df = df[[\"created_at\", \"id\", \"full_text\", \"geo\", \"coordinates\", \"place\", \"retweet_count\", \"favorite_count\"]]\n",
    "    \n",
    "    # get state\n",
    "    states = []\n",
    "    \n",
    "    for location_dict in df[\"place\"]:\n",
    "        try:\n",
    "            states.append(get_state_from_location(location_dict))\n",
    "        except Exception as e:\n",
    "            print(location_dict)\n",
    "            print(e)\n",
    "            \n",
    "    df[\"US_state\"] = states\n",
    "    \n",
    "    # get dates of tweets\n",
    "    dates = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hours = []\n",
    "\n",
    "    for timestamp in df[\"created_at\"]:\n",
    "        hour = pd.to_datetime(timestamp).hour\n",
    "        dt_obj = pd.to_datetime(timestamp).date()\n",
    "        year = dt_obj.year\n",
    "        month = dt_obj.month\n",
    "        day = dt_obj.day\n",
    "\n",
    "        hours.append(hour)\n",
    "        months.append(month)\n",
    "        days.append(day)\n",
    "\n",
    "        if month < 10:\n",
    "            month = f\"0{month}\"\n",
    "\n",
    "        dates.append(f\"{year}-{month}-{day}\")\n",
    "\n",
    "    df[\"date_of_tweet\"] = dates\n",
    "    df[\"month_of_tweet\"] = months\n",
    "    df[\"day_of_tweet\"] = days\n",
    "    df[\"hour_of_tweet\"] = hours\n",
    "\n",
    "    # clean the text\n",
    "    df[\"cleaned_text\"] = df[\"full_text\"].apply(clean_text)\n",
    "\n",
    "    # work with hashtags\n",
    "    hashtags_arr = []\n",
    "    num_hashtags_arr = []\n",
    "    text_no_hashtags_arr = []\n",
    "\n",
    "    for tokenized_text in df[\"cleaned_text\"]:\n",
    "        hashtag_lst = []\n",
    "        text_no_hashtags_lst = []\n",
    "\n",
    "        for word in tokenized_text:\n",
    "            if '#' in word:\n",
    "                hashtag_lst.append(word)\n",
    "            else:\n",
    "                text_no_hashtags_lst.append(word)\n",
    "\n",
    "        hashtags_arr.append(hashtag_lst)\n",
    "        num_hashtags_arr.append(len(hashtag_lst))\n",
    "        text_no_hashtags_arr.append(text_no_hashtags_lst)\n",
    "\n",
    "    df[\"hashtags\"] = hashtags_arr\n",
    "    df[\"hashtags_count\"] = num_hashtags_arr\n",
    "    df[\"cleaned_text_no_hashtags\"] = text_no_hashtags_arr\n",
    "\n",
    "    # get only cols that we care about\n",
    "    df_small = df[[\"id\", \"full_text\", \"retweet_count\", \"favorite_count\", \"place\", \n",
    "                   \"US_state\", \"date_of_tweet\", \"month_of_tweet\", \"day_of_tweet\", \n",
    "                   \"hour_of_tweet\", \"cleaned_text\", \"hashtags\", \"hashtags_count\", \"cleaned_text_no_hashtags\"]]\n",
    "\n",
    "    return df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDRATED_TWEETS_DIR = \"../../data/tweets/hydrated_tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"2020-12-22_2021-01-09_tweets.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Export tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = \"../../data/tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(EXPORT_DIR + \"tweets_2020-12-22_2021-01-09_with_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_venv",
   "language": "python",
   "name": "twitter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
