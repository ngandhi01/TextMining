{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of this file\n",
    "\n",
    "In this file, I will hydrate tweets from this resource: https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset\n",
    "\n",
    "This resource contains a subset of tweets, scraped every day since March 20th, that have geolocation data. \n",
    "\n",
    "We can use code from \"get_location_from_geocoordinates.py\" in order to see how to use lat/long info to get a person's location. \n",
    "\n",
    "For this first pass, we'll use the following dates:\n",
    "\n",
    "1. (NOTE: not using this date, since for this dataset we don't have data from this date) March 9th: Governor DeSantis declares a State of Emergency\n",
    "2. April 17th: DeSantis issues a statewide stay-at-home order following growing pressure to do so\n",
    "3. May 18th: DeSantis says that Florida will begin full phase one of reopening, allowing gyms and restaurants to operate at 50% capacity, starting May 18.\n",
    "4. June 5th: DeSantis announces that Florida could move into Phase 2 except south Florida, specifically Miami-Dade, Broward, and Palm Beach, which need to submit plans for reopening. Phase 2 in Florida begins, with bars allowed to open at 50% capacity with social distancing and sanitation.\n",
    "5. July 2nd: Florida reports 10,000 new coronavirus cases in a single day, the biggest one-day increase in the state since the pandemic started, and more than any European country had at the height of their outbreaks.\n",
    "6. September 25th: Governor Ron DeSantis fully opened the state of Florida by executive order on Friday. The order also prohibits local governments from imposing fines or shutting down businesses, or enforcing mask mandates\n",
    "7. October 17th: Florida reported its highest COVID19 numbers in two onths. The seven-day average was more than 3,300 cases. Reporting anomalies made it more difficult to gather statistical trends. Positivity rate was 5.2%, with over 2,000 hospitalizations. \n",
    "8. December 17th:  Florida reported 13,148 new cases, largest since July 16th\n",
    "\n",
    "All these dates correspond with important COVID-related events in Florida. I chose Florida since it's had a large range of different COVID-related events (e.g., openings, closings, shutdowns, etc.), rather than some other states that, say, had an initial lockdown and stayed in lockdown. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime as datetime\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "pd.set_option('display.max_columns', None) # show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load tweets\n",
    "\n",
    "Due to sharing restrictions, the public dataset doesn't have the actual tweets themselves. Rather, it has the tweet IDs. Therefore, we can \"hydrate\" the tweet IDs to recover the actual tweets\n",
    "\n",
    "(Also, accessing the tweets requires an IEEE account, so the link might not work in the future? Accessing the tweets is easy with the website link above, however). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect tweets from April 16th to April 17th\n",
    "april16_17 = pd.read_csv(\"https://ieee-dataport.s3.amazonaws.com/open/14206/april16_april17.csv?response-content-disposition=attachment%3B%20filename%3D%22april16_april17.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223856Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=c4fbf41e249dc1fb9f5f7be962a2ca05d5210d0a9a81293820f49c91efed3826\", \n",
    "                         names = [\"tweet_id\", \"sentiment_score\"])\n",
    "\n",
    "# collect tweets from April 17th to April 18th\n",
    "april17_18 = pd.read_csv(\"https://ieee-dataport.s3.amazonaws.com/open/14206/april17_april18.csv?response-content-disposition=attachment%3B%20filename%3D%22april17_april18.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223856Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=db65792c9a01221fd2e70f90dfa5dcbc2b5fd9649311e7ee6b11e810c69c0c60\", \n",
    "                          names = [\"tweet_id\", \"sentiment_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "april17 = pd.concat([april16_17, april17_18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "april17.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can export these tweet IDs in a .csv file, and then we can use twarc, a command line Python tool, to get the tweets that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = list(april17[\"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_ID_DIR = \"../../data/tweets/tweet_ids/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TWEET_ID_DIR + \"april17_tweets.csv\", 'a+') as f: # a+ lets us both append and write\n",
    "    for idx, tweet in enumerate(tweet_ids):\n",
    "        if idx != len(tweet_ids) - 1:\n",
    "            f.write(f\"{tweet},\\n\")\n",
    "        else:\n",
    "            f.write(f\"{tweet}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these tweet IDs, let's hydrate them to recover the original tweets\n",
    "\n",
    "First, you have to confirm your credentials. \n",
    "\n",
    "`twarc configure`\n",
    "\n",
    "Then, submit the creds. After doing so successfully, you should get a message like this: \n",
    "\n",
    "`The credentials for default have been saved to your configuration file at /Users/mark/.twarc`\n",
    "\n",
    "Afterwards, you can start hydrating the tweets. \n",
    "\n",
    "This can be done in the command line\n",
    "\n",
    "You'd run something like this:\n",
    "\n",
    "`twarc hydrate ids.txt > tweets.jsonl`\n",
    "\n",
    "In my case, running the command from the root directory of this project, it looks something like this:\n",
    "\n",
    "`twarc hydrate data/tweets/tweet_ids/april17_tweets.csv > data/tweets/hydrated_tweets/april17_tweets.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDRATED_TWEETS_DIR = \"../../data/tweets/hydrated_tweets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the .jsonl file into a .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonObj = pd.read_json(path_or_buf=HYDRATED_TWEETS_DIR + \"april17_tweets.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start processing tweets, getting the info that we care about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We likely only care about the following columns:\n",
    "\n",
    "    • created_at\n",
    "    • id\n",
    "    • full_text\n",
    "    • geo\n",
    "    • coordinates\n",
    "    • place (this has the city + state location, as a field called \"full_name\")\n",
    "    • retweet_count\n",
    "    • favorite_count\n",
    "    \n",
    "We also want to parse the \"created_at\" column (we can perhaps create 2 columns, one with the date and one with the hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = jsonObj[[\"created_at\", \"id\", \"full_text\", \"geo\", \"coordinates\", \"place\", \"retweet_count\", \"favorite_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each tweet, let's get the states that they're in. We have a `place` column that has a dictionary with place information. For the tweets in the USA, we can get state-level information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_location(place):\n",
    "    \"\"\"\n",
    "    Gets state info from place field\n",
    "    Assumes dict input\n",
    "    \"\"\"\n",
    "    \n",
    "    if place is None:\n",
    "        state = \"NA\"  \n",
    "    elif place[\"country_code\"] != \"US\":\n",
    "        state = \"NA\"\n",
    "    else:\n",
    "        state = place[\"full_name\"].split(\",\")[1].strip() # e.g., \"Los Angeles, CA\" --> \"CA\"\n",
    "        \n",
    "    return state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location_dict in df[\"place\"]:\n",
    "    try:\n",
    "        states.append(get_state_from_location(location_dict))\n",
    "    except Exception as e:\n",
    "        print(location_dict)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"US_state\"] = states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have locations, let's also get the dates of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "months = []\n",
    "days = []\n",
    "hours = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format = \"2020-04-16\"\n",
    "\n",
    "for timestamp in df[\"created_at\"]:\n",
    "    hour = pd.to_datetime(timestamp).hour\n",
    "    dt_obj = pd.to_datetime(timestamp).date()\n",
    "    year = dt_obj.year\n",
    "    month = dt_obj.month\n",
    "    day = dt_obj.day\n",
    "    \n",
    "    hours.append(hour)\n",
    "    months.append(month)\n",
    "    days.append(day)\n",
    "    \n",
    "    if month < 10:\n",
    "        month = f\"0{month}\"\n",
    "    \n",
    "    dates.append(f\"{year}-{month}-{day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df[\"date_of_tweet\"] = dates\n",
    "df[\"month_of_tweet\"] = months\n",
    "df[\"day_of_tweet\"] = days\n",
    "df[\"hour_of_tweet\"] = hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse the full text and check for counts of certain words as well. \n",
    "\n",
    "Let's do the following:\n",
    "\n",
    "Cleaning steps:\n",
    "\n",
    "1. Remove punctuation\n",
    "2. Do string split\n",
    "3. Remove links\n",
    "\n",
    "Processing:\n",
    "1. Make all the words lowercase\n",
    "2. Remove stopwords\n",
    "3. Stem/lemmatize (maybe?)\n",
    "\n",
    "Then, for analysis,\n",
    "\n",
    "1. Create a new column for all the hashtags (and add all the hashtags, per tweet)\n",
    "2. Do word counts of specific words\n",
    "3. Do LDA/topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION ='''!()-[]{};:'\"\\,<>./?@$%^&*_~''' # keep hashtags\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    text = text.encode(\"utf-8\")\n",
    "    allchars = [str for str in text.decode('utf-8')]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Removes punctuation, does string split, and removes links\n",
    "    \"\"\"\n",
    "    \n",
    "    return_arr = []\n",
    "    \n",
    "    # remove punctuation\n",
    "    text_no_punctuation = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char not in PUNCTUATION:\n",
    "            text_no_punctuation = text_no_punctuation + char\n",
    "            \n",
    "    # remove emojis\n",
    "    text_no_punctuation = remove_emoji(text_no_punctuation)\n",
    "    text_no_punctuation = re.sub(r'\\\\U[a-zA-Z0-9]{8}', '', text_no_punctuation)\n",
    "    \n",
    "    # remove \\n and \\t\n",
    "    text_no_punctuation = re.sub(r'\\n', '', text_no_punctuation)\n",
    "    text_no_punctuation = re.sub(r'\\t', '', text_no_punctuation)\n",
    "    \n",
    "    # remove escape sequences\n",
    "    text_no_escape = \"\"\n",
    "    \n",
    "    for char in text_no_punctuation:\n",
    "        try:\n",
    "            char.encode('ascii')\n",
    "            text_no_escape = text_no_escape + char # this'll catch chars that don't have an ascii equivalent (e.g., emojis)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # add space between # and another char before it (e.g., split yes#baseball into yes #baseball)\n",
    "    text_no_escape = re.sub(r\"([a-zA-Z0-9]){1}#\", r\"\\1 #\", text_no_escape)\n",
    "    \n",
    "    # other preprocessing\n",
    "    text_arr = text_no_escape.split(' ')\n",
    "    \n",
    "    for word in text_arr:\n",
    "        \n",
    "        # clean words\n",
    "        word = word.lower()\n",
    "        \n",
    "        if \"http\" not in word and word.strip() != '' and word not in STOPWORDS:\n",
    "            return_arr.append(word)\n",
    "            \n",
    "    return return_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"cleaned_text\"] = df[\"full_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deal with hashtags. Let's create a new column that contains all the hashtags, a column that counts how many hashtags there are, and a third column that has the text array (tokenized text) without hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_arr = []\n",
    "num_hashtags_arr = []\n",
    "text_no_hashtags_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenized_text in df[\"cleaned_text\"]:\n",
    "    hashtag_lst = []\n",
    "    text_no_hashtags_lst = []\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        if '#' in word:\n",
    "            hashtag_lst.append(word)\n",
    "        else:\n",
    "            text_no_hashtags_lst.append(word)\n",
    "    \n",
    "    hashtags_arr.append(hashtag_lst)\n",
    "    num_hashtags_arr.append(len(hashtag_lst))\n",
    "    text_no_hashtags_arr.append(text_no_hashtags_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/mark/opt/anaconda3/envs/twitter_venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df[\"hashtags\"] = hashtags_arr\n",
    "df[\"hashtags_count\"] = num_hashtags_arr\n",
    "df[\"cleaned_text_no_hashtags\"] = text_no_hashtags_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>cleaned_text_no_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[finally, got, color, love, length, im, okay, ...</td>\n",
       "      <td>[#quarantine, #covid, #corona, #haircolor, #ha...</td>\n",
       "      <td>11</td>\n",
       "      <td>[finally, got, color, love, length, im, okay, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[#wutang, #wutangforever, #corona, downtown, l...</td>\n",
       "      <td>[#wutang, #wutangforever, #corona]</td>\n",
       "      <td>3</td>\n",
       "      <td>[downtown, los, angeles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[swirling, corona, california]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[swirling, corona, california]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[feel, like, want, #crawl, walls, get, heres, ...</td>\n",
       "      <td>[#crawl, #quarantine, #staygolden, #staysafe, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[feel, like, want, walls, get, heres, dont, dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[get, stanky, booty, cave, creek, amp, carefre...</td>\n",
       "      <td>[#walmart, #cavecreek, #toiletpaper, #quaranti...</td>\n",
       "      <td>6</td>\n",
       "      <td>[get, stanky, booty, cave, creek, amp, carefre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>[whats, say, cap, #swipeleft, #swipeleft, #run...</td>\n",
       "      <td>[#swipeleft, #swipeleft, #runnersofinstagram, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>[whats, say, cap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>[#musicphillpromotions, #never, leave, home, w...</td>\n",
       "      <td>[#musicphillpromotions, #never, #mask, #protec...</td>\n",
       "      <td>8</td>\n",
       "      <td>[leave, home, without, virus, jamaica, kingsto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>[home, workout, hamstringy, like, gym, still, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[home, workout, hamstringy, like, gym, still, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>[thank, much, #lukebryan, playing, dock, coron...</td>\n",
       "      <td>[#lukebryan, #countrymusic, #country, #music, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>[thank, much, playing, dock, corona, virus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>[finallybeen, rough, april, 2020, expect, one,...</td>\n",
       "      <td>[#rtr, #covid19survivor]</td>\n",
       "      <td>2</td>\n",
       "      <td>[finallybeen, rough, april, 2020, expect, one,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_text  \\\n",
       "0    [finally, got, color, love, length, im, okay, ...   \n",
       "1    [#wutang, #wutangforever, #corona, downtown, l...   \n",
       "2                       [swirling, corona, california]   \n",
       "3    [feel, like, want, #crawl, walls, get, heres, ...   \n",
       "4    [get, stanky, booty, cave, creek, amp, carefre...   \n",
       "..                                                 ...   \n",
       "793  [whats, say, cap, #swipeleft, #swipeleft, #run...   \n",
       "794  [#musicphillpromotions, #never, leave, home, w...   \n",
       "795  [home, workout, hamstringy, like, gym, still, ...   \n",
       "796  [thank, much, #lukebryan, playing, dock, coron...   \n",
       "797  [finallybeen, rough, april, 2020, expect, one,...   \n",
       "\n",
       "                                              hashtags  hashtags_count  \\\n",
       "0    [#quarantine, #covid, #corona, #haircolor, #ha...              11   \n",
       "1                   [#wutang, #wutangforever, #corona]               3   \n",
       "2                                                   []               0   \n",
       "3    [#crawl, #quarantine, #staygolden, #staysafe, ...               9   \n",
       "4    [#walmart, #cavecreek, #toiletpaper, #quaranti...               6   \n",
       "..                                                 ...             ...   \n",
       "793  [#swipeleft, #swipeleft, #runnersofinstagram, ...              14   \n",
       "794  [#musicphillpromotions, #never, #mask, #protec...               8   \n",
       "795                                                 []               0   \n",
       "796  [#lukebryan, #countrymusic, #country, #music, ...              13   \n",
       "797                           [#rtr, #covid19survivor]               2   \n",
       "\n",
       "                              cleaned_text_no_hashtags  \n",
       "0    [finally, got, color, love, length, im, okay, ...  \n",
       "1                             [downtown, los, angeles]  \n",
       "2                       [swirling, corona, california]  \n",
       "3    [feel, like, want, walls, get, heres, dont, dr...  \n",
       "4    [get, stanky, booty, cave, creek, amp, carefre...  \n",
       "..                                                 ...  \n",
       "793                                  [whats, say, cap]  \n",
       "794  [leave, home, without, virus, jamaica, kingsto...  \n",
       "795  [home, workout, hamstringy, like, gym, still, ...  \n",
       "796        [thank, much, playing, dock, corona, virus]  \n",
       "797  [finallybeen, rough, april, 2020, expect, one,...  \n",
       "\n",
       "[798 rows x 4 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"cleaned_text\", \"hashtags\", \"hashtags_count\", \"cleaned_text_no_hashtags\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the columns that we'll actually use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>US_state</th>\n",
       "      <th>date_of_tweet</th>\n",
       "      <th>month_of_tweet</th>\n",
       "      <th>day_of_tweet</th>\n",
       "      <th>hour_of_tweet</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>cleaned_text_no_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-16 04:26:07+00:00</td>\n",
       "      <td>1250641596887990272</td>\n",
       "      <td>Finally got to a color I love and a length I’m...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [44.15253213,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-72.5981195,...</td>\n",
       "      <td>{'id': '495a55057ac886b9', 'url': 'https://api...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VT</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[finally, got, color, love, length, im, okay, ...</td>\n",
       "      <td>[#quarantine, #covid, #corona, #haircolor, #ha...</td>\n",
       "      <td>11</td>\n",
       "      <td>[finally, got, color, love, length, im, okay, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-16 04:46:25+00:00</td>\n",
       "      <td>1250646705516707840</td>\n",
       "      <td>#wutang #wutangforever #corona @ Downtown Los ...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [34.03742524,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-118.2487404...</td>\n",
       "      <td>{'id': '3b77caf94bfc81fe', 'url': 'https://api...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[#wutang, #wutangforever, #corona, downtown, l...</td>\n",
       "      <td>[#wutang, #wutangforever, #corona]</td>\n",
       "      <td>3</td>\n",
       "      <td>[downtown, los, angeles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-16 04:47:43+00:00</td>\n",
       "      <td>1250647034253709315</td>\n",
       "      <td>Swirling again @ Corona, California https://t....</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [33.8753, -11...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-117.566, 33...</td>\n",
       "      <td>{'id': '5e4b6834e36e68fa', 'url': 'https://api...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[swirling, corona, california]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[swirling, corona, california]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-16 05:19:41+00:00</td>\n",
       "      <td>1250655078744240134</td>\n",
       "      <td>Does it feel like you want to #Crawl on the wa...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [59.3307, 18....</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [18.0605, 59....</td>\n",
       "      <td>{'id': 'd56c5babcffde8ef', 'url': 'https://api...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>[feel, like, want, #crawl, walls, get, heres, ...</td>\n",
       "      <td>[#crawl, #quarantine, #staygolden, #staysafe, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[feel, like, want, walls, get, heres, dont, dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-16 05:21:19+00:00</td>\n",
       "      <td>1250655491904147456</td>\n",
       "      <td>Get your stanky booty to Cave Creek &amp;amp; Care...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [33.8304, -11...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-111.964, 33...</td>\n",
       "      <td>{'id': '005e9bd60c4f1337', 'url': 'https://api...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>[get, stanky, booty, cave, creek, amp, carefre...</td>\n",
       "      <td>[#walmart, #cavecreek, #toiletpaper, #quaranti...</td>\n",
       "      <td>6</td>\n",
       "      <td>[get, stanky, booty, cave, creek, amp, carefre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id  \\\n",
       "0 2020-04-16 04:26:07+00:00  1250641596887990272   \n",
       "1 2020-04-16 04:46:25+00:00  1250646705516707840   \n",
       "2 2020-04-16 04:47:43+00:00  1250647034253709315   \n",
       "3 2020-04-16 05:19:41+00:00  1250655078744240134   \n",
       "4 2020-04-16 05:21:19+00:00  1250655491904147456   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Finally got to a color I love and a length I’m...   \n",
       "1  #wutang #wutangforever #corona @ Downtown Los ...   \n",
       "2  Swirling again @ Corona, California https://t....   \n",
       "3  Does it feel like you want to #Crawl on the wa...   \n",
       "4  Get your stanky booty to Cave Creek &amp; Care...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0  {'type': 'Point', 'coordinates': [44.15253213,...   \n",
       "1  {'type': 'Point', 'coordinates': [34.03742524,...   \n",
       "2  {'type': 'Point', 'coordinates': [33.8753, -11...   \n",
       "3  {'type': 'Point', 'coordinates': [59.3307, 18....   \n",
       "4  {'type': 'Point', 'coordinates': [33.8304, -11...   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0  {'type': 'Point', 'coordinates': [-72.5981195,...   \n",
       "1  {'type': 'Point', 'coordinates': [-118.2487404...   \n",
       "2  {'type': 'Point', 'coordinates': [-117.566, 33...   \n",
       "3  {'type': 'Point', 'coordinates': [18.0605, 59....   \n",
       "4  {'type': 'Point', 'coordinates': [-111.964, 33...   \n",
       "\n",
       "                                               place  retweet_count  \\\n",
       "0  {'id': '495a55057ac886b9', 'url': 'https://api...              0   \n",
       "1  {'id': '3b77caf94bfc81fe', 'url': 'https://api...              0   \n",
       "2  {'id': '5e4b6834e36e68fa', 'url': 'https://api...              0   \n",
       "3  {'id': 'd56c5babcffde8ef', 'url': 'https://api...              0   \n",
       "4  {'id': '005e9bd60c4f1337', 'url': 'https://api...              0   \n",
       "\n",
       "   favorite_count US_state date_of_tweet  month_of_tweet  day_of_tweet  \\\n",
       "0               0       VT    2020-04-16               4            16   \n",
       "1               0       CA    2020-04-16               4            16   \n",
       "2               0       CA    2020-04-16               4            16   \n",
       "3               0       NA    2020-04-16               4            16   \n",
       "4               1       AZ    2020-04-16               4            16   \n",
       "\n",
       "   hour_of_tweet                                       cleaned_text  \\\n",
       "0              4  [finally, got, color, love, length, im, okay, ...   \n",
       "1              4  [#wutang, #wutangforever, #corona, downtown, l...   \n",
       "2              4                     [swirling, corona, california]   \n",
       "3              5  [feel, like, want, #crawl, walls, get, heres, ...   \n",
       "4              5  [get, stanky, booty, cave, creek, amp, carefre...   \n",
       "\n",
       "                                            hashtags  hashtags_count  \\\n",
       "0  [#quarantine, #covid, #corona, #haircolor, #ha...              11   \n",
       "1                 [#wutang, #wutangforever, #corona]               3   \n",
       "2                                                 []               0   \n",
       "3  [#crawl, #quarantine, #staygolden, #staysafe, ...               9   \n",
       "4  [#walmart, #cavecreek, #toiletpaper, #quaranti...               6   \n",
       "\n",
       "                            cleaned_text_no_hashtags  \n",
       "0  [finally, got, color, love, length, im, okay, ...  \n",
       "1                           [downtown, los, angeles]  \n",
       "2                     [swirling, corona, california]  \n",
       "3  [feel, like, want, walls, get, heres, dont, dr...  \n",
       "4  [get, stanky, booty, cave, creek, amp, carefre...  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[[\"id\", \"full_text\", \"retweet_count\", \"favorite_count\", \"place\",\n",
    "               \"US_state\", \"date_of_tweet\", \"month_of_tweet\", \"day_of_tweet\", \n",
    "               \"hour_of_tweet\", \"cleaned_text\", \"hashtags\", \"hashtags_count\", \"cleaned_text_no_hashtags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "april17_tweets = df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these steps finalized, let's do this for the other tweets in the dates of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform same steps as above, for other tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Load tweets\n",
    "\n",
    "We have to get tweets from the following dates:\n",
    "\n",
    "1. May 18th\n",
    "2. June 5th\n",
    "3. July 2nd\n",
    "4. September 25th\n",
    "5. October 17th\n",
    "6. December 17th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_to_hydrate(link1, link2, csv_name, tweet_id_dir = TWEET_ID_DIR):\n",
    "    \n",
    "    \"\"\"\n",
    "        Takes the links to both of the csv files for the given date, as well as name of export file\n",
    "        \n",
    "        e.g., for may 18th, we need the csv file for may17th-may18th tweets as well as may18th-may19th tweets\n",
    "        \n",
    "        Assumes that directory for tweet IDs is specified\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_csv(link1, names=[\"tweet_id\", \"sentiment_score\"])\n",
    "    df2 = pd.read_csv(link2, names=[\"tweet_id\", \"sentiment_score\"])\n",
    "    \n",
    "    df = pd.concat([df1, df2])\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    tweet_ids = list(df[\"tweet_id\"])\n",
    "    \n",
    "    with open(tweet_id_dir + csv_name, \"a+\") as f:\n",
    "        for idx, tweet in enumerate(tweet_ids):\n",
    "            if idx != len(tweet_ids) - 1:\n",
    "                f.write(f\"{tweet}, \\n\")\n",
    "            else:\n",
    "                f.write(f\"{tweet}\")\n",
    "                \n",
    "    print(f\"CSV file {csv_name} successfully exported\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the links for each set of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "may18_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/may17_may18.csv?response-content-disposition=attachment%3B%20filename%3D%22may17_may18.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223856Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=6c8e8d872f76ffd5ddf941d4acbb5d6697026d39f7f9647384bf241cbf59a05e\", \n",
    "               \"https://ieee-dataport.s3.amazonaws.com/open/14206/may18_may19.csv?response-content-disposition=attachment%3B%20filename%3D%22may18_may19.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223856Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=b34bb6b06dc250f7c8db140228acf955fc1f32560d5e9c446b951f1be32a7c44\"]\n",
    "\n",
    "june5_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/june4_june5.csv?response-content-disposition=attachment%3B%20filename%3D%22june4_june5.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=a2ddb2e400f6b3325ac74d78e1076f41c232fdce8d954c56cf05fd1ddec21477\", \n",
    "               \"https://ieee-dataport.s3.amazonaws.com/open/14206/june5_june6.csv?response-content-disposition=attachment%3B%20filename%3D%22june5_june6.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=5a1c812ea08809675086d31805f41c00f3db67f235c9d54d2a21412da2050b64\"]\n",
    "\n",
    "july2_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/july1_july2.csv?response-content-disposition=attachment%3B%20filename%3D%22july1_july2.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=02334a59ffb1d2de0b277490ea46dbaef8e157888c208707411cfc4fe14c158b\", \n",
    "               \"https://ieee-dataport.s3.amazonaws.com/open/14206/july2_july3.csv?response-content-disposition=attachment%3B%20filename%3D%22july2_july3.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=1f899ae6d28282276a90ff2a7a38cbf80141f80b49f230931954a7d40c37f2d6\"]\n",
    "\n",
    "september25_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/september24_september25.csv?response-content-disposition=attachment%3B%20filename%3D%22september24_september25.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=71033cb9a413f9ad0143cb3c9db0e7c1634a6896a8aa8fd0780a18ad51f039a1\", \n",
    "                     \"https://ieee-dataport.s3.amazonaws.com/open/14206/september25_september26.csv?response-content-disposition=attachment%3B%20filename%3D%22september25_september26.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=d5e42d1ae42bf43b7486ef653b3af9d41e8212ac2a4e9389ea3b497d638a2323\"]\n",
    "\n",
    "october17_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/october16_october17.csv?response-content-disposition=attachment%3B%20filename%3D%22october16_october17.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7456e8331dd4f8118b3b86c2e6cd9147cf4897b14d1dc36508da36410d878f17\", \n",
    "                   \"https://ieee-dataport.s3.amazonaws.com/open/14206/october17_october18.csv?response-content-disposition=attachment%3B%20filename%3D%22october17_october18.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223857Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=c21e10b10ee754de11af61717095de844a4b3453a717c0ee1c04679e3e28d8a5\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file may18_tweets.csv successfully exported\n",
      "CSV file june5_tweets.csv successfully exported\n",
      "CSV file july2_tweets.csv successfully exported\n",
      "CSV file september25_tweets.csv successfully exported\n",
      "CSV file october17_tweets.csv successfully exported\n"
     ]
    }
   ],
   "source": [
    "get_tweets_to_hydrate(may18_links[0], may18_links[1], \"may18_tweets.csv\")\n",
    "get_tweets_to_hydrate(june5_links[0], june5_links[1], \"june5_tweets.csv\")\n",
    "get_tweets_to_hydrate(july2_links[0], july2_links[1], \"july2_tweets.csv\")\n",
    "get_tweets_to_hydrate(september25_links[0], september25_links[1], \"september25_tweets.csv\")\n",
    "get_tweets_to_hydrate(october17_links[0], october17_links[1], \"october17_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll hydrate these in terminal. \n",
    "\n",
    "The command, assuming that it's run from the project root directory, is: \n",
    "\n",
    "`twarc hydrate data/tweets/tweet_ids/april17_tweets.csv > data/tweets/hydrated_tweets/april17_tweets.jsonl`\n",
    "\n",
    "So, the commands would be:\n",
    "\n",
    "twarc hydrate data/tweets/tweet_ids/may18_tweets.csv > data/tweets/hydrated_tweets/may18_tweets.jsonl\n",
    "\n",
    "twarc hydrate data/tweets/tweet_ids/june5_tweets.csv > data/tweets/hydrated_tweets/june5_tweets.jsonl\n",
    "\n",
    "twarc hydrate data/tweets/tweet_ids/july2_tweets.csv > data/tweets/hydrated_tweets/july2_tweets.jsonl\n",
    "\n",
    "twarc hydrate data/tweets/tweet_ids/september25_tweets.csv > data/tweets/hydrated_tweets/september25_tweets.jsonl\n",
    "\n",
    "twarc hydrate data/tweets/tweet_ids/october17_tweets.csv > data/tweets/hydrated_tweets/october17_tweets.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the hydrated tweets, let's encapsulate all our steps above into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hydrated_tweets(tweet_jsonl_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        Takes .jsonl from Twitter, returns the cleaned df\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # get uncleaned df from json\n",
    "    df = pd.read_json(path_or_buf=tweet_jsonl_path, lines=True)\n",
    "    df = df[[\"created_at\", \"id\", \"full_text\", \"geo\", \"coordinates\", \"place\", \"retweet_count\", \"favorite_count\"]]\n",
    "    \n",
    "    # get state\n",
    "    states = []\n",
    "    \n",
    "    for location_dict in df[\"place\"]:\n",
    "        try:\n",
    "            states.append(get_state_from_location(location_dict))\n",
    "        except Exception as e:\n",
    "            print(location_dict)\n",
    "            print(e)\n",
    "            \n",
    "    df[\"US_state\"] = states\n",
    "    \n",
    "    # get dates of tweets\n",
    "    dates = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hours = []\n",
    "\n",
    "    for timestamp in df[\"created_at\"]:\n",
    "        hour = pd.to_datetime(timestamp).hour\n",
    "        dt_obj = pd.to_datetime(timestamp).date()\n",
    "        year = dt_obj.year\n",
    "        month = dt_obj.month\n",
    "        day = dt_obj.day\n",
    "\n",
    "        hours.append(hour)\n",
    "        months.append(month)\n",
    "        days.append(day)\n",
    "\n",
    "        if month < 10:\n",
    "            month = f\"0{month}\"\n",
    "\n",
    "        dates.append(f\"{year}-{month}-{day}\")\n",
    "\n",
    "    df[\"date_of_tweet\"] = dates\n",
    "    df[\"month_of_tweet\"] = months\n",
    "    df[\"day_of_tweet\"] = days\n",
    "    df[\"hour_of_tweet\"] = hours\n",
    "\n",
    "    # clean the text\n",
    "    df[\"cleaned_text\"] = df[\"full_text\"].apply(clean_text)\n",
    "\n",
    "    # work with hashtags\n",
    "    hashtags_arr = []\n",
    "    num_hashtags_arr = []\n",
    "    text_no_hashtags_arr = []\n",
    "\n",
    "    for tokenized_text in df[\"cleaned_text\"]:\n",
    "        hashtag_lst = []\n",
    "        text_no_hashtags_lst = []\n",
    "\n",
    "        for word in tokenized_text:\n",
    "            if '#' in word:\n",
    "                hashtag_lst.append(word)\n",
    "            else:\n",
    "                text_no_hashtags_lst.append(word)\n",
    "\n",
    "        hashtags_arr.append(hashtag_lst)\n",
    "        num_hashtags_arr.append(len(hashtag_lst))\n",
    "        text_no_hashtags_arr.append(text_no_hashtags_lst)\n",
    "\n",
    "    df[\"hashtags\"] = hashtags_arr\n",
    "    df[\"hashtags_count\"] = num_hashtags_arr\n",
    "    df[\"cleaned_text_no_hashtags\"] = text_no_hashtags_arr\n",
    "\n",
    "    # get only cols that we care about\n",
    "    df_small = df[[\"id\", \"full_text\", \"retweet_count\", \"favorite_count\", \"place\", \n",
    "                   \"US_state\", \"date_of_tweet\", \"month_of_tweet\", \"day_of_tweet\", \n",
    "                   \"hour_of_tweet\", \"cleaned_text\", \"hashtags\", \"hashtags_count\", \"cleaned_text_no_hashtags\"]]\n",
    "\n",
    "    return df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the function for all of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "may18_tweets = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"may18_tweets.jsonl\")\n",
    "june5_tweets = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"june5_tweets.jsonl\")\n",
    "july2_tweets = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"july2_tweets.jsonl\")\n",
    "september25_tweets = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"september25_tweets.jsonl\")\n",
    "october17_tweets = clean_hydrated_tweets(HYDRATED_TWEETS_DIR + \"october17_tweets.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine all our tweets into one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([april17_tweets, may18_tweets, june5_tweets, \n",
    "                         july2_tweets, september25_tweets, october17_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPLETE THE COLLECTION, once 2020-12-18 tweets come out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this collection was done 2020-12-17 and 2020-12-18, so we don't have tweets yet for 2020-12-18\n",
    "december17_links = [\"https://ieee-dataport.s3.amazonaws.com/open/14206/december16_december17.csv?response-content-disposition=attachment%3B%20filename%3D%22december16_december17.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223858Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=fcdd2d0ff9309266bfb5a3836070956a000cc52ab316f0e88f43220d7a91be09\",\n",
    "                    \"https://ieee-dataport.s3.amazonaws.com/open/14206/december16_december17.csv?response-content-disposition=attachment%3B%20filename%3D%22december16_december17.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20201217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201217T223858Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=fcdd2d0ff9309266bfb5a3836070956a000cc52ab316f0e88f43220d7a91be09\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EXPORT ALL TWEETS (redo when 2020-12-18 tweets come out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = \"../../data/tweets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.tweet_scrape_2020-11-30_control.json.icloud',\n",
       " '.DS_Store',\n",
       " '.tweet_scrape_2020_11_25.json.icloud',\n",
       " 'tweet_ids',\n",
       " '.tweet_scrape_2020-11-30_treatment.csv.icloud',\n",
       " '.tweet_scrape_2020-11-30_treatment.json.icloud',\n",
       " '.tweet_scrape_2020-11-30_control.csv.icloud',\n",
       " '.tweet_scrape_2020_11_25.csv.icloud',\n",
       " 'hydrated_tweets']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../../data/tweets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(EXPORT_DIR + \"tweets_2020-12-18_with_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_venv",
   "language": "python",
   "name": "twitter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
